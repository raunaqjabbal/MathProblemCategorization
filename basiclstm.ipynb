{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ***Importing Libraries***","metadata":{}},{"cell_type":"code","source":"!pip install textattack -q -q -q --exists-action i\n\n\nimport pandas as pd\nimport numpy as np \nimport sys\nfrom textattack.augmentation import EasyDataAugmenter\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import wordnet\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Input, Dropout, GRU, Lambda, Conv1D, MaxPooling1D\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T04:32:39.375382Z","iopub.execute_input":"2023-06-22T04:32:39.376174Z","iopub.status.idle":"2023-06-22T04:34:05.449844Z","shell.execute_reply.started":"2023-06-22T04:32:39.376139Z","shell.execute_reply":"2023-06-22T04:34:05.448889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Using NLTK and TextAttack for Augmentation***","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw')\n! unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n! unzip /usr/share/nltk_data/corpora/omw.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-06-22T04:38:34.366946Z","iopub.execute_input":"2023-06-22T04:38:34.367358Z","iopub.status.idle":"2023-06-22T04:38:37.011358Z","shell.execute_reply.started":"2023-06-22T04:38:34.367322Z","shell.execute_reply":"2023-06-22T04:38:37.010329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM = 8","metadata":{"execution":{"iopub.status.busy":"2023-06-22T04:34:05.451686Z","iopub.execute_input":"2023-06-22T04:34:05.453907Z","iopub.status.idle":"2023-06-22T04:34:05.458969Z","shell.execute_reply.started":"2023-06-22T04:34:05.453870Z","shell.execute_reply":"2023-06-22T04:34:05.457458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textattack.augmentation import EasyDataAugmenter\naugmenter = EasyDataAugmenter(pct_words_to_swap=0.6,transformations_per_example=NUM)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T04:38:49.010985Z","iopub.execute_input":"2023-06-22T04:38:49.011491Z","iopub.status.idle":"2023-06-22T04:38:51.424965Z","shell.execute_reply.started":"2023-06-22T04:38:49.011436Z","shell.execute_reply":"2023-06-22T04:38:51.424033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Importing Data***","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/math-problem-categorization/train.csv')\ntest = pd.read_csv('/kaggle/input/math-problem-categorization/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/math-problem-categorization/sample_submission.csv')\n\n\nprint(train.head())\nprint()\nprint(train.info())\nprint()\n\nprint(train.duplicated().value_counts())     ## Checking for duplicate entries\nprint()\n\nprint(\"Invalid Enties: \",(train['category']).isnull().sum())     ## Checking for invalid entries\nprint()","metadata":{"execution":{"iopub.status.busy":"2023-06-22T04:38:51.426906Z","iopub.execute_input":"2023-06-22T04:38:51.427533Z","iopub.status.idle":"2023-06-22T04:38:51.499847Z","shell.execute_reply.started":"2023-06-22T04:38:51.427497Z","shell.execute_reply":"2023-06-22T04:38:51.498856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Splitting Data into Train, Val & Test sets & Preprocessing Code***","metadata":{}},{"cell_type":"code","source":"import re\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('[0-9]+', '#', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-06-22T04:38:54.105976Z","iopub.execute_input":"2023-06-22T04:38:54.106348Z","iopub.status.idle":"2023-06-22T04:38:54.113138Z","shell.execute_reply.started":"2023-06-22T04:38:54.106316Z","shell.execute_reply":"2023-06-22T04:38:54.111767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = list(train['problem'].apply(clean_text))\ny_train = train['category'].to_numpy()\nx_test = list(test[\"problem\"].apply(clean_text))\n\nnum_classes = len(np.unique(y_train))      ","metadata":{"execution":{"iopub.status.busy":"2023-06-22T05:01:36.592003Z","iopub.execute_input":"2023-06-22T05:01:36.592392Z","iopub.status.idle":"2023-06-22T05:01:36.606925Z","shell.execute_reply.started":"2023-06-22T05:01:36.592360Z","shell.execute_reply":"2023-06-22T05:01:36.605768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class preprocess():\n    def __init__(self):\n        self.tokenizer = None\n        self.max_len = None\n        \n    def __call__(self, corpus, y=None):\n        if self.tokenizer == None:\n            newcorpus=[]\n            newlabels=[]\n            for problem, category in zip(corpus, y):\n                demo = [problem] + augmenter.augment(problem)\n                newcorpus+= demo\n                newlabels+= [category] * len(demo)\n                print(len(newcorpus), len(newlabels))\n\n            corpus = newcorpus\n            y = newlabels\n            \n            self.tokenizer = Tokenizer(oov_token= '<OOV>', char_level = False, filters = '')     ## Tokenize input sentence\n            self.tokenizer.fit_on_texts(corpus) \n            self.total_words = len(self.tokenizer.word_index) + 1                                 ## Vocabulary Size\n            self.max_len = len(max(corpus, key=len))                                              ## Length of longest sentence\n        \n        corpus = self.tokenizer.texts_to_sequences(corpus)\n        corpus = pad_sequences(corpus, maxlen = self.max_len, padding = 'pre')\n    \n        if y==None:\n            return corpus\n        else:\n            return [corpus,y] ","metadata":{"execution":{"iopub.status.busy":"2023-06-22T05:01:38.100773Z","iopub.execute_input":"2023-06-22T05:01:38.101117Z","iopub.status.idle":"2023-06-22T05:01:38.110797Z","shell.execute_reply.started":"2023-06-22T05:01:38.101088Z","shell.execute_reply":"2023-06-22T05:01:38.109774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Final Augmented Data***","metadata":{}},{"cell_type":"code","source":"preprocessor = preprocess()\nx_train, y_train = preprocessor(x_train, y_train)\n\nnew_x_test = []\nfor problem in x_test:\n    new_x_test+= [problem] + augmenter.augment(str(problem))            \nx_test = new_x_test\n\nx_test = preprocessor(x_test)\n\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, stratify = y_train)\nx_train = np.array(x_train)\ny_train = np.array(y_train)\nx_val = np.array(x_val)\ny_val = np.array(y_val)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-22T05:01:39.585002Z","iopub.execute_input":"2023-06-22T05:01:39.585693Z","iopub.status.idle":"2023-06-22T05:23:46.937326Z","shell.execute_reply.started":"2023-06-22T05:01:39.585659Z","shell.execute_reply":"2023-06-22T05:23:46.936345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Model Checkpoint for saving most suitable weights***","metadata":{}},{"cell_type":"code","source":"model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath = 'checkpoints/weights.{epoch:02d}.hdf5',\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max')","metadata":{"execution":{"iopub.status.busy":"2023-06-22T05:34:09.890789Z","iopub.execute_input":"2023-06-22T05:34:09.891183Z","iopub.status.idle":"2023-06-22T05:34:09.896840Z","shell.execute_reply.started":"2023-06-22T05:34:09.891153Z","shell.execute_reply":"2023-06-22T05:34:09.895595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Getting Glove Embeddings***","metadata":{}},{"cell_type":"code","source":"# !wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n# !unzip glove*.zip","metadata":{"execution":{"iopub.status.busy":"2023-06-21T20:25:55.838800Z","iopub.status.idle":"2023-06-21T20:25:55.839627Z","shell.execute_reply.started":"2023-06-21T20:25:55.839374Z","shell.execute_reply":"2023-06-21T20:25:55.839399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-06-22T05:34:14.542451Z","iopub.execute_input":"2023-06-22T05:34:14.542876Z","iopub.status.idle":"2023-06-22T05:34:15.113169Z","shell.execute_reply.started":"2023-06-22T05:34:14.542844Z","shell.execute_reply":"2023-06-22T05:34:15.112124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_FILE = '/kaggle/working/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in   open(EMBEDDING_FILE))\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = -0.005838499,0.48782197\nembed_size = all_embs.shape[1]","metadata":{"execution":{"iopub.status.busy":"2023-06-22T05:34:16.143420Z","iopub.execute_input":"2023-06-22T05:34:16.145583Z","iopub.status.idle":"2023-06-22T05:37:15.679932Z","shell.execute_reply.started":"2023-06-22T05:34:16.145542Z","shell.execute_reply":"2023-06-22T05:37:15.678919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_glove(word_index):\n    nb_words = min(max_features, len(word_index)+1)\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n        else:\n            embedding_vector = embeddings_index.get(word.capitalize())\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n\n\nmax_features = preprocessor.total_words\nembedding_matrix = load_glove(preprocessor.tokenizer.word_index)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T05:37:15.681882Z","iopub.execute_input":"2023-06-22T05:37:15.682235Z","iopub.status.idle":"2023-06-22T05:37:15.750250Z","shell.execute_reply.started":"2023-06-22T05:37:15.682202Z","shell.execute_reply":"2023-06-22T05:37:15.749346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Model Architecture***","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features, 300,embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix)))\n\nmodel.add(Bidirectional(LSTM(32, dropout = 0.5)))\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-06-22T07:19:35.877513Z","iopub.execute_input":"2023-06-22T07:19:35.877890Z","iopub.status.idle":"2023-06-22T07:19:36.430527Z","shell.execute_reply.started":"2023-06-22T07:19:35.877858Z","shell.execute_reply":"2023-06-22T07:19:36.429576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Training Model***","metadata":{}},{"cell_type":"code","source":"history = model.fit(x_train, y_train, epochs = 60, validation_data=(x_val, y_val), callbacks= [model_checkpoint_callback])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-22T07:19:37.910617Z","iopub.execute_input":"2023-06-22T07:19:37.910991Z","iopub.status.idle":"2023-06-22T07:23:54.322725Z","shell.execute_reply.started":"2023-06-22T07:19:37.910960Z","shell.execute_reply":"2023-06-22T07:23:54.321795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Plotting History***","metadata":{}},{"cell_type":"code","source":"plt.style.use('ggplot')\nplt.rcParams[\"figure.figsize\"] = (16,4)\nmylen = range(len(history.history[\"loss\"]))\n\nplt.subplot(1,2,1)\nplt.plot(mylen, history.history[\"loss\"], label=\"Train\")\nplt.plot(mylen, history.history[\"val_loss\"], label=\"Val\")\nplt.title(\"Loss\")\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(mylen, history.history[\"accuracy\"], label= \"Train\")\nplt.plot(mylen, history.history[\"val_accuracy\"], label=\"Val\")\nplt.title(\"Accuracy\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-22T07:23:58.434523Z","iopub.execute_input":"2023-06-22T07:23:58.435645Z","iopub.status.idle":"2023-06-22T07:23:58.924681Z","shell.execute_reply.started":"2023-06-22T07:23:58.435610Z","shell.execute_reply":"2023-06-22T07:23:58.923138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Loading Most Suitable Weights***","metadata":{}},{"cell_type":"code","source":"model.load_weights('checkpoints/weights.40.hdf5')","metadata":{"execution":{"iopub.status.busy":"2023-06-22T07:24:10.869829Z","iopub.execute_input":"2023-06-22T07:24:10.870211Z","iopub.status.idle":"2023-06-22T07:24:10.901296Z","shell.execute_reply.started":"2023-06-22T07:24:10.870178Z","shell.execute_reply":"2023-06-22T07:24:10.900349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Augmenting Test Dataset***\n\nEach sample will be augmented multiple times, and i'll take mode of these pedictions.","metadata":{}},{"cell_type":"code","source":"x_test = list(test[\"problem\"].apply(clean_text))\nnew_x_test=[]\nfor problem in x_test:\n    demo= [problem] + augmenter.augment(str(problem))            \n    demo = preprocessor(demo)\n    new_x_test+= [[demo,len(demo)]]\n    print([demo.shape,len(demo)])\n    print(\"New Length:\", len(new_x_test))\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-06-22T06:25:11.118115Z","iopub.execute_input":"2023-06-22T06:25:11.118489Z","iopub.status.idle":"2023-06-22T06:32:41.320189Z","shell.execute_reply.started":"2023-06-22T06:25:11.118452Z","shell.execute_reply":"2023-06-22T06:32:41.319053Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Predicting Results***","metadata":{}},{"cell_type":"code","source":"soln=[]\nfor sample,length in new_x_test:\n    total_preds = np.argmax(model(sample),axis = 1).reshape(-1,length)\n    ans = stats.mode(total_preds,axis = 1, keepdims= False)[0].reshape(-1)\n    print(ans, end=\"\")\n    soln += [ans[0]]","metadata":{"execution":{"iopub.status.busy":"2023-06-22T07:24:19.782033Z","iopub.execute_input":"2023-06-22T07:24:19.782464Z","iopub.status.idle":"2023-06-22T07:24:23.323475Z","shell.execute_reply.started":"2023-06-22T07:24:19.782420Z","shell.execute_reply":"2023-06-22T07:24:23.322264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['category'] = soln\ntest.to_csv(\"Raunaq.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T07:24:36.356125Z","iopub.execute_input":"2023-06-22T07:24:36.356511Z","iopub.status.idle":"2023-06-22T07:24:36.365529Z","shell.execute_reply.started":"2023-06-22T07:24:36.356473Z","shell.execute_reply":"2023-06-22T07:24:36.364426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from scipy import stats\n# total_preds = np.argmax(model(x_test),axis = 1).reshape(-1,NUM)\n# total_preds = stats.mode(total_preds,axis = 1)[0].reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2023-06-22T06:08:55.150849Z","iopub.status.idle":"2023-06-22T06:08:55.151695Z","shell.execute_reply.started":"2023-06-22T06:08:55.151445Z","shell.execute_reply":"2023-06-22T06:08:55.151468Z"},"trusted":true},"execution_count":null,"outputs":[]}]}